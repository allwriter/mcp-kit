# mcp-kit
  
### ❓MCP 서버가 뭐야
- 정의: LLM(예: ChatGPT) 같은 AI 모델이 외부 기능이나 데이터를 활용할 수 있게 해주는 중간 서버입니다.
- 역할: 챗봇이 “바로 실행할 수 없는 기능”을 대신 처리해 주는 플러그인 서버라고 보면 됩니다.
- 즉, MCP 서버 = LLM을 위한 확장 기능 서버
</br>

### ❓LLM은 어떻게 MCP 함수를 알고, 그 규격대로 호출해
- MCP 서버는 자신이 제공하는 기능을 “스펙(spec)” 형태로 LLM에게 알려줍니다.
- MCP 서버는 “내가 지원하는 함수(action)와 파라미터, 반환 형식”을 LLM이 이해할 수 있게 Capability 등록을 합니다.
```
{
  "capabilities": [
    {
      "name": "get_user",
      "description": "사용자 정보를 조회합니다.",
      "parameters": {
        "id": "integer"
      }
    },
    {
      "name": "ping",
      "description": "서버 응답 확인",
      "parameters": {}
    }
  ]
}
```
</br>

### ❓아 그럼 LLM은 Description을 직접 해석해서 사용하는거야
- 네, 맞아요 LLM은 description을 직접 해석해서, 사용자의 질문과 MCP 기능을 연결합니다.
- description이 부실하면 LLM이 오해해서 틀린 함수 호출을 할 수 있어요.
- 그래서 좋은 MCP 서버 = “잘 짜여진 API” + “LLM 친화적인 설명” 두 가지가 동시에 필요합니다. 
</br>

### ❓ChatGPT한테 내가 개발한 MCP를 붙여서 사용할 수 있나
- 현재 제 환경(여기서 실행되는 ChatGPT)은 외부 네트워크에 임의로 접속하거나, 임의 서버와 소켓 연결을 맺을 권한이 없어요.
- 따라서 제가 만든 요청을 직접 당신의 MCP 서버에 보내는 방식(테스트 클라이언트처럼)은 불가능합니다.
      
✅ 하지만 가능한 방법
- 당신이 MCP 서버를 로컬이나 클라우드에 띄움
- MCP 클라이언트/프록시를 ChatGPT 같은 LLM 앱에 붙임
- 현재 OpenAI의 ChatGPT에서도 MCP 지원이 점차 열리고 있어요.
- 예를 들어 VS Code 확장, LangChain, LlamaIndex 같은 프레임워크에서 MCP 서버를 붙일 수 있습니다.
</br>

### ❓그럼 내가 테스트할 챗봇은 직접 만들어야해
- 당신이 만든 MCP 서버를 테스트하려면 "챗봇 클라이언트"가 필요합니다.
  
✅ MCP 서버 테스트 방법  
1) 직접 호출 → 빠르고 쉽지만 “LLM이 description을 읽고 판단”하는 부분은 테스트가 안 됨  
2) LLM 클라이언트 직접 구현하여 테스트
3) 이미 있는 프레임워크 활용 → LangChain, VS Code MCP 클라이언트(OpenAI가 공식 지원 중)
</br>

### ❓챗봇 만들어 이용하는 것과 VS Code 클라이언트 이용하는 것에 어떤 차이가 있나
- 최소 기능 챗봇 클라이언트
장점: 테스트 환경을 완전히 커스터마이징 가능  
단점: 직접 구축·운영 부담 있음  
  
- VS Code MCP 클라이언트
장점: 세팅만 하면 바로 사용 가능, UI 완비  
단점: 자유도는 상대적으로 제한적 (VS Code 환경에 종속)
</br>

### ❓그럼 내가 MCP서버만 빠르게 개발 테스트해야 하는 입장이라면 직접 챗봇을 만들거나 LangChain 같은 프레임워크를 사용하는 것보다 VS Code 클라이언트를 사용하는 것이 좋겠네
- MCP 서버 동작 확인이 목적이라면 VS Code MCP 클라이언트가 제일 효율적이고,
- MCP를 실제 서비스에 녹이기가 목적이라면 직접 챗봇이나 LangChain 같은 프레임워크가 맞습니다.  
👌
  
